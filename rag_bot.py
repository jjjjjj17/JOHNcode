# åŒ¯å…¥ç›¸é—œå¥—ä»¶
import requests
from bs4 import BeautifulSoup
from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_community.chat_models import ChatOpenAI
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
import os

# ç›´æ¥åœ¨ç¨‹å¼ç¢¼ä¸­å¯«å…¥ API keyï¼ˆè«‹æ›æˆä½ çš„å¯¦éš› keyï¼‰
OPENAI_API_KEY = ""

# === ç¬¬ä¸€æ­¥ï¼šçˆ¬å–ç¶²ç«™æ–‡ç«  ===
def crawl_articles():
    url = "https://technews.tw/"
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")

    # å–å¾—æ–‡ç« æ¨™é¡Œèˆ‡é€£çµ
    links = soup.select("h1.entry-title a")
    articles = []
    for link in links[:5]:  # åªæŠ“å‰5ç¯‡
        title = link.text.strip()
        href = link['href']
        article = get_article_content(href)
        articles.append(title + "\n" + article)
    return articles

# === ç¬¬äºŒæ­¥ï¼šæŠ“å–æ–‡ç« å…§å®¹ ===
def get_article_content(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(res.text, "html.parser")
        # æŠ“æ­£æ–‡å…§å®¹
        content = soup.select_one("div.entry-content")
        return content.get_text(strip=True).replace("\n", "")[:1000]  # é™åˆ¶å­—æ•¸é¿å…å¤ªé•·
    except Exception as e:
        return ""

# === ç¬¬ä¸‰æ­¥ï¼šåˆ‡ç‰‡ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº« ===
def build_vector_db(articles):
    docs = [Document(page_content=a) for a in articles]
    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = splitter.split_documents(docs)

    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)  # å‚³å…¥ API key
    vectordb = FAISS.from_documents(split_docs, embeddings)
    return vectordb

# === ç¬¬å››æ­¥ï¼šå»ºç«‹å•ç­”ç³»çµ± ===
def create_qa_chain(vectordb):
    retriever = vectordb.as_retriever()
    qa_chain = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(temperature=0.2, model="gpt-3.5-turbo", openai_api_key=OPENAI_API_KEY),
        retriever=retriever
    )
    return qa_chain

# === ç¬¬äº”æ­¥ï¼šä¸»æµç¨‹ ===
def main():
    print("ğŸ“¥ é–‹å§‹çˆ¬å–æœ€æ–°æŠ€è¡“æ–°è...")
    articles = crawl_articles()
    print(f"âœ… æˆåŠŸæŠ“å– {len(articles)} ç¯‡æ–‡ç« ")

    print("ğŸ”§ å»ºç«‹å‘é‡è³‡æ–™åº«ä¸­...")
    vectordb = build_vector_db(articles)

    print("ğŸ¤– å•ç­”æ©Ÿå™¨äººå·²å°±ç·’ï¼è«‹è¼¸å…¥ä½ çš„å•é¡Œï¼š")
    qa = create_qa_chain(vectordb)

    while True:
        query = input("ğŸ“Œ å•é¡Œï¼ˆè¼¸å…¥ 'exit' é›¢é–‹ï¼‰ï¼š")
        if query.lower() == "exit":
            break
        answer = qa.run(query)
        print("ğŸ’¡ AI å›ç­”ï¼š", answer)

if __name__ == "__main__":
    main()
