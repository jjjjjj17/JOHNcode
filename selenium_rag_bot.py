from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import time
import requests

# === è¨­å®š Chrome Driver ===
CHROMEDRIVER_PATH = r"C:\PYTHONC\chromedriver-win64\chromedriver.exe"  # â† æ”¹æˆä½ çš„å¯¦éš›è·¯å¾‘

def start_browser():
    options = Options()
    options.add_argument("--headless")  # ç„¡é ­æ¨¡å¼ï¼ˆä¸è·³å‡ºè¦–çª—ï¼‰
    options.add_argument("--disable-gpu")
    service = Service(CHROMEDRIVER_PATH)
    driver = webdriver.Chrome(service=service, options=options)
    return driver

# === å‹•æ…‹çˆ¬å– TechNews é¦–é æ–‡ç« é€£çµ ===
def crawl_articles():
    driver = start_browser()
    driver.get("https://technews.tw/")
    time.sleep(2)  # ç­‰å¾… JS è¼‰å…¥å…§å®¹

    # å–å¾—é é¢ HTML è§£æ
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    links = soup.select("h1.entry-title a")
    articles = []
    for link in links[:5]:
        title = link.text.strip()
        href = link['href']
        article = get_article_content(href)
        articles.append(title + "\n" + article)
    return articles

# === æ–‡ç« å…§å®¹æ“·å–ï¼ˆrequests å³å¯ï¼‰ ===
def get_article_content(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(res.text, "html.parser")
        content = soup.select_one("div.entry-content")
        return content.get_text(strip=True).replace("\n", "")[:1000]
    except Exception as e:
        return ""

# === ä¸»ç¨‹å¼æ¸¬è©¦ ===
if __name__ == "__main__":
    print("ğŸš€ å•Ÿå‹• Selenium å‹•æ…‹çˆ¬èŸ²...")
    articles = crawl_articles()
    print(f"âœ… æˆåŠŸæ“·å– {len(articles)} ç¯‡æ–‡ç« \n")

    for i, a in enumerate(articles, 1):
        print(f"--- æ–‡ç«  {i} ---")
        print(a[:300] + "...\n")
